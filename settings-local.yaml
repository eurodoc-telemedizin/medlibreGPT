# poetry install --extras "ui llms-llama-cpp vector-stores-qdrant embeddings-huggingface"
server:
  env_name: ${APP_ENV:local}

llm:
  mode: llamacpp
  max_new_tokens: 1024         # 4096 is fine but start smaller while debugging
  context_window: 8192         # align with Mixtral-Instructâ€™s 8k
  tokenizer: mistralai/Mixtral-8x7B-Instruct-v0.1
  prompt_style: "mistral"      # keep this; privateGPT uses this style

llamacpp:
  llm_hf_repo_id: TheBloke/Mixtral-8x7B-v0.1-GGUF
  llm_hf_model_file: mixtral-8x7b-v0.1.Q6_K.gguf
  # Ensure the runtime knows the chat format if GGUF lacks a template:
  chat_format: mistral-instruct
  n_ctx: 8192
  # Good defaults (tune to your hardware):
  n_threads: 8
  n_gpu_layers: 0              # set >0 if you have GPU offload
  # Make sure generation stops cleanly:
  stop: ["</s>", "[/INST]", "<|eot_id|>"]
  add_bos_token: true

rag:
  prompt:
    instruction: ""   # or remove this node entirely


embedding:
  mode: huggingface

huggingface:
  embedding_hf_model_name: nomic-ai/nomic-embed-text-v1.5

vectorstore:
  database: qdrant

qdrant:
  path: local_data/private_gpt/qdrant
